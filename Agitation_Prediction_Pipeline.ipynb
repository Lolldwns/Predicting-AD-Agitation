{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 100\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND INITIAL SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"Load training and test datasets.\"\"\"\n",
    "    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "    return train_df, test_df\n",
    "\n",
    "def create_validation_split(train_df, n_patients=5):\n",
    "    \"\"\"Create validation set by randomly selecting patients.\"\"\"\n",
    "    train_df_full = train_df.copy()\n",
    "    \n",
    "    # Randomly choose patients for validation\n",
    "    pids = np.random.choice(train_df['subject'].unique(), size=n_patients, replace=False)\n",
    "    valid_df = pd.DataFrame()\n",
    "    for pid in pids:\n",
    "        valid_df = pd.concat([valid_df, train_df.loc[train_df['subject'] == pid]])\n",
    "    \n",
    "    train_df = train_df[~train_df['subject'].isin(pids)]\n",
    "    return train_df, valid_df, train_df_full\n",
    "\n",
    "# =============================================================================\n",
    "# DATA AGGREGATION\n",
    "# =============================================================================\n",
    "\n",
    "def aggregate_function(df, keys, global_agg_fn='max'):\n",
    "    \"\"\"Aggregate hourly data to daily format using specified aggregation function.\"\"\"\n",
    "    \n",
    "    # Perform global aggregation\n",
    "    if global_agg_fn == 'sum':\n",
    "        agg_df = df.groupby(keys).agg(lambda x: x.sum() if not x.isna().all() else np.nan)\n",
    "    else:\n",
    "        agg_df = df.groupby(keys).agg(global_agg_fn) \n",
    "\n",
    "    # Handle categorical feature separately (mode aggregation)\n",
    "    agg_df['global_tib'] = df.groupby(keys)['global_tib'].agg(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else None\n",
    "    ) \n",
    "\n",
    "    # Drop the 'hour' column\n",
    "    agg_df = agg_df.drop(['hour'], axis=1)\n",
    "    return agg_df\n",
    "\n",
    "def aggregate_datasets(train_df, valid_df, test_df, train_df_full):\n",
    "    \"\"\"Aggregate all datasets and convert labels to boolean.\"\"\"\n",
    "    keys = ['subject', 'date']\n",
    "    \n",
    "    # Aggregate datasets\n",
    "    agg_train_df = aggregate_function(train_df, keys=keys)\n",
    "    agg_valid_df = aggregate_function(valid_df, keys=keys)\n",
    "    agg_test_df = aggregate_function(test_df, keys=keys)\n",
    "    agg_train_full_df = aggregate_function(train_df_full, keys=keys)\n",
    "    \n",
    "    # Convert labels to boolean (any agitation during day = True)\n",
    "    agg_train_df['label'] = agg_train_df['label'] > 0\n",
    "    agg_valid_df['label'] = agg_valid_df['label'] > 0\n",
    "    agg_train_full_df['label'] = agg_train_full_df['label'] > 0\n",
    "    \n",
    "    return agg_train_df, agg_valid_df, agg_test_df, agg_train_full_df\n",
    "\n",
    "# =============================================================================\n",
    "# DATA IMPUTATION\n",
    "# =============================================================================\n",
    "\n",
    "def impute_missing_values(df, cols):\n",
    "    \"\"\"Impute missing values using iterative imputation for specified columns.\"\"\"\n",
    "    imputed_df = df.copy()\n",
    "    \n",
    "    # Initialize and apply iterative imputer\n",
    "    imputer = IterativeImputer(random_state=SEED)\n",
    "    imputed_df[cols] = imputer.fit_transform(imputed_df[cols])\n",
    "    \n",
    "    # Fill remaining missing activity values with zero\n",
    "    imputed_df = imputed_df.fillna(0)\n",
    "    \n",
    "    return imputed_df\n",
    "\n",
    "# =============================================================================\n",
    "# DATA TRANSFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "def transform_categorical_features(df_list):\n",
    "    \"\"\"Transform categorical variables and apply one-hot encoding.\"\"\"\n",
    "    transformed_dfs = []\n",
    "    \n",
    "    for df in df_list:\n",
    "        df_copy = df.copy()\n",
    "        df_copy[\"global_tib\"] = df_copy[\"global_tib\"].astype('category')\n",
    "        transformed_dfs.append(df_copy)\n",
    "    \n",
    "    # Get categories from training data\n",
    "    categories = transformed_dfs[0][\"global_tib\"].cat.categories\n",
    "    \n",
    "    # Convert to codes and apply one-hot encoding\n",
    "    for i, df in enumerate(transformed_dfs):\n",
    "        if i == 0:  # Training data\n",
    "            df[\"global_tib\"] = df[\"global_tib\"].cat.codes\n",
    "        else:  # Other datasets\n",
    "            df[\"global_tib\"] = pd.Categorical(df[\"global_tib\"], categories=categories).codes\n",
    "        \n",
    "        # One-hot encode\n",
    "        transformed_dfs[i] = pd.get_dummies(df, columns=['global_tib'])\n",
    "    \n",
    "    # Ensure consistent columns across datasets\n",
    "    train_global_tib_cols = [col for col in transformed_dfs[0].columns if 'global_tib' in col]\n",
    "    \n",
    "    for i in range(1, len(transformed_dfs)):\n",
    "        df_global_tib_cols = [col for col in transformed_dfs[i].columns if 'global_tib' in col]\n",
    "        missing_cols = [col for col in train_global_tib_cols if col not in df_global_tib_cols]\n",
    "        \n",
    "        for col in missing_cols:\n",
    "            transformed_dfs[i][col] = False\n",
    "    \n",
    "    return transformed_dfs\n",
    "\n",
    "def convert_to_boolean(df_list, bool_cols):\n",
    "    \"\"\"Convert specified integer columns to boolean.\"\"\"\n",
    "    for df in df_list:\n",
    "        for col in bool_cols:\n",
    "            df[col] = df[col].apply(lambda x: True if x > 0 else False)\n",
    "    return df_list\n",
    "\n",
    "def visualize_feature_distributions(df, scaling_cols):\n",
    "    \"\"\"Create histograms for numerical features to understand distributions.\"\"\"\n",
    "    fig, axes = plt.subplots(10, 2, figsize=(25, 30))\n",
    "    colors = plt.rcParams[\"axes.prop_cycle\"]()\n",
    "    \n",
    "    for ax, col in zip(np.ravel(axes), scaling_cols):\n",
    "        ax.hist(df[col], color=next(colors)[\"color\"], bins=20)\n",
    "        ax.set_title(f\"{col.title()} Histogram\")\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_xlabel('Value')\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.75, wspace=0.25)\n",
    "    plt.show()\n",
    "\n",
    "def check_outliers(df, scaling_cols, threshold=3):\n",
    "    \"\"\"Check for outliers using Z-score method.\"\"\"\n",
    "    z = np.abs(stats.zscore(df[scaling_cols]))\n",
    "    outliers = (z > threshold)\n",
    "    total_outliers = np.sum(outliers)\n",
    "    \n",
    "    print(f\"Number of outliers: {total_outliers}\")\n",
    "    print(f\"Total Number of Datapoints: {df[scaling_cols].size}\")\n",
    "    print(f\"Percentage of Outliers: {total_outliers/df[scaling_cols].size * 100:.2f}%\")\n",
    "    \n",
    "    return total_outliers\n",
    "\n",
    "def apply_scaling(df_list, scaling_cols):\n",
    "    \"\"\"Apply MinMax scaling to numerical features.\"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_dfs = []\n",
    "    \n",
    "    # Fit scaler on training data and transform all datasets\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        if i == 0:  # Training data\n",
    "            df_copy[scaling_cols] = scaler.fit_transform(df_copy[scaling_cols].values)\n",
    "        else:  # Other datasets\n",
    "            df_copy[scaling_cols] = scaler.transform(df_copy[scaling_cols].values)\n",
    "        scaled_dfs.append(df_copy)\n",
    "    \n",
    "    return scaled_dfs\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def select_features_with_random_forest(X, y, n_estimators=100):\n",
    "    \"\"\"Select important features using Random Forest feature importance.\"\"\"\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=n_estimators, random_state=SEED))\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    importance_series = pd.Series(\n",
    "        selector.estimator_.feature_importances_.ravel(),\n",
    "        index=X.columns\n",
    "    )\n",
    "    importance_series.plot.bar(figsize=(10, 5))\n",
    "    plt.ylabel('Feature importance')\n",
    "    plt.title('Feature Importance from Random Forest')\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_features, selector\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL TRAINING AND EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def balance_dataset_with_smote(X, y):\n",
    "    \"\"\"Balance dataset using SMOTE oversampling.\"\"\"\n",
    "    smote = SMOTE(random_state=SEED)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    print(f'Original dataset shape: {Counter(y)}')\n",
    "    print(f'Resampled dataset shape: {Counter(y_resampled)}')\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def evaluate_models(models, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train multiple models and evaluate their performance.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[str(model)] = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'precision': precision_score(y_val, y_pred),\n",
    "            'recall': recall_score(y_val, y_pred),\n",
    "            'f1_score': f1_score(y_val, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"Accuracy: {results[str(model)]['accuracy']*100:.2f}%\")\n",
    "        print(f\"Precision: {results[str(model)]['precision']:.2f}\")\n",
    "        print(f\"Recall: {results[str(model)]['recall']:.2f}\")\n",
    "        print(f\"F1 Score: {results[str(model)]['f1_score']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def cross_validate_model(model, X, y, cv_folds=5):\n",
    "    \"\"\"Perform k-fold cross-validation on a model.\"\"\"\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=SEED)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    \n",
    "    print(f\"K-Fold Cross-Validation Accuracy Scores: {scores}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(scores):.3f}\")\n",
    "    print(f\"Standard Deviation: {np.std(scores):.3f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    data_path = '/path/to/your/data'  # Update this path\n",
    "    \n",
    "    # Columns for different processing steps\n",
    "    imputation_cols = [\n",
    "        'flag_heart_day', 'flag_pressure_day',\n",
    "        'max diastolic blood pressure', 'max heart rate', 'max systolic blood pressure', \n",
    "        'min diastolic blood pressure', 'min heart rate', 'min systolic blood pressure',\n",
    "        'average diastolic blood pressure', 'average heart rate', 'average systolic blood pressure', \n",
    "        'heart rate', 'breathing rate', 'snoring', \n",
    "        'time to bed', 'time to rise', \n",
    "        'time in bed', 'time out of bed'\n",
    "    ]\n",
    "    \n",
    "    scaling_cols = [\n",
    "        'bathroom', 'bedroom', 'door', 'kitchen', 'other_location',\n",
    "        'max diastolic blood pressure', 'max heart rate', 'max systolic blood pressure', \n",
    "        'min diastolic blood pressure', 'min heart rate', 'min systolic blood pressure',\n",
    "        'average diastolic blood pressure', 'average heart rate', 'average systolic blood pressure', \n",
    "        'heart rate', 'breathing rate', \n",
    "        'time to bed', 'time to rise', \n",
    "        'time in bed', 'time out of bed'\n",
    "    ]\n",
    "    \n",
    "    bool_cols = ['snoring', 'flag_heart_day', 'flag_pressure_day']\n",
    "    \n",
    "    # Step 1: Load and split data\n",
    "    print(\"Loading data...\")\n",
    "    train_df, test_df = load_data(data_path)\n",
    "    train_df, valid_df, train_df_full = create_validation_split(train_df)\n",
    "    \n",
    "    print(f\"Training patients: {train_df['subject'].nunique()}\")\n",
    "    print(f\"Test patients: {test_df['subject'].nunique()}\")\n",
    "    \n",
    "    # Step 2: Aggregate data\n",
    "    print(\"\\nAggregating data...\")\n",
    "    agg_train_df, agg_valid_df, agg_test_df, agg_train_full_df = aggregate_datasets(\n",
    "        train_df, valid_df, test_df, train_df_full\n",
    "    )\n",
    "    \n",
    "    # Step 3: Handle missing values\n",
    "    print(\"\\nImputing missing values...\")\n",
    "    imputed_train_df = impute_missing_values(agg_train_df, imputation_cols)\n",
    "    imputed_valid_df = impute_missing_values(agg_valid_df, imputation_cols)\n",
    "    imputed_test_df = impute_missing_values(agg_test_df, imputation_cols)\n",
    "    imputed_train_full_df = impute_missing_values(agg_train_full_df, imputation_cols)\n",
    "    \n",
    "    # Step 4: Transform features\n",
    "    print(\"\\nTransforming features...\")\n",
    "    df_list = [imputed_train_df, imputed_valid_df, imputed_test_df, imputed_train_full_df]\n",
    "    transformed_dfs = transform_categorical_features(df_list)\n",
    "    transformed_dfs = convert_to_boolean(transformed_dfs, bool_cols)\n",
    "    \n",
    "    # Step 5: Scale features\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaled_dfs = apply_scaling(transformed_dfs, scaling_cols)\n",
    "    final_train_df, final_valid_df, final_test_df, final_train_full_df = scaled_dfs\n",
    "    \n",
    "    # Step 6: Prepare data for modeling\n",
    "    print(\"\\nPreparing data for modeling...\")\n",
    "    train_X = final_train_df.drop('label', axis=1)\n",
    "    train_y = final_train_df['label']\n",
    "    valid_X = final_valid_df.drop('label', axis=1)\n",
    "    valid_y = final_valid_df['label']\n",
    "    train_full_X = final_train_full_df.drop('label', axis=1)\n",
    "    train_full_y = final_train_full_df['label']\n",
    "    \n",
    "    # Step 7: Balance dataset\n",
    "    print(\"\\nBalancing dataset with SMOTE...\")\n",
    "    smote_train_X, smote_train_y = balance_dataset_with_smote(train_X, train_y)\n",
    "    smote_train_full_X, smote_train_full_y = balance_dataset_with_smote(train_full_X, train_full_y)\n",
    "    \n",
    "    # Step 8: Feature selection\n",
    "    print(\"\\nSelecting features...\")\n",
    "    smote_train_X_df = pd.DataFrame(smote_train_X, columns=train_X.columns)\n",
    "    selected_features, selector = select_features_with_random_forest(smote_train_X_df, smote_train_y)\n",
    "    \n",
    "    print(f\"Selected features: {list(selected_features)}\")\n",
    "    \n",
    "    # Apply feature selection\n",
    "    important_features = ['bathroom', 'bedroom', 'door', 'kitchen', 'time to bed', 'other_location']\n",
    "    smote_train_X_selected = smote_train_X_df[important_features].values\n",
    "    valid_X_selected = valid_X[important_features].values\n",
    "    smote_train_full_X_selected = pd.DataFrame(smote_train_full_X, columns=train_full_X.columns)[important_features].values\n",
    "    test_X_selected = final_test_df[important_features].values\n",
    "    \n",
    "    # Step 9: Train and evaluate models\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    models = [\n",
    "        KNeighborsClassifier(),\n",
    "        RandomForestClassifier(random_state=SEED),\n",
    "        LogisticRegression(random_state=SEED),\n",
    "        GradientBoostingClassifier(random_state=SEED),\n",
    "        SVC(kernel='poly', random_state=SEED),\n",
    "        GaussianNB(var_smoothing=0.5)\n",
    "    ]\n",
    "    \n",
    "    results = evaluate_models(models, smote_train_X_selected, smote_train_y, \n",
    "                            valid_X_selected, valid_y)\n",
    "    \n",
    "    # Step 10: Select best model and make predictions\n",
    "    print(\"\\nSelecting best model (KNN) and making final predictions...\")\n",
    "    best_model = KNeighborsClassifier(\n",
    "        n_neighbors=5, weights='uniform', algorithm='auto', \n",
    "        metric='euclidean', leaf_size=30, p=2\n",
    "    )\n",
    "    \n",
    "    # Cross-validate the best model\n",
    "    cross_validate_model(best_model, smote_train_full_X_selected, smote_train_full_y)\n",
    "    \n",
    "    # Train on full dataset and predict\n",
    "    best_model.fit(smote_train_full_X_selected, smote_train_full_y)\n",
    "    test_predictions = best_model.predict(test_X_selected)\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions_df = pd.DataFrame(test_predictions, columns=['Predicted'])\n",
    "    predictions_df.to_csv('test_preds.csv', index=False)\n",
    "    print(\"Predictions saved to 'test_preds.csv'\")\n",
    "    \n",
    "    return best_model, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update the data_path in the main() function before running\n",
    "    model, evaluation_results = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
